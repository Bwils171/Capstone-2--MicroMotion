{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import librosa as lib\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio\n",
    "import scipy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Scheduler: \"tcp://127.0.0.1:56999\" processes: 3 cores: 6>,\n",
       " {0: <Nanny: tcp://127.0.0.1:57016, threads: 2>,\n",
       "  1: <Nanny: tcp://127.0.0.1:57022, threads: 2>,\n",
       "  2: <Nanny: tcp://127.0.0.1:57019, threads: 2>})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "import joblib\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "cluster.scheduler, cluster.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_pickle('DFs/model_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scaled = pd.DataFrame(scaler.fit_transform(model_data.drop(columns=['segment', 'SID', 'PID'])), \n",
    "#                       columns=model_data.drop(columns=['segment', 'SID', 'PID']).columns)\n",
    "# with joblib.parallel_backend('dask'):\n",
    "#     pca.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exdf = pd.DataFrame(ex.reshape(1, -1), columns=range(len(ex)))\n",
    "# exdf.T.rename(columns={0:'variance_explained'}).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prep\\all_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prep\\all_prep.py \n",
    "\n",
    "def Xy_prep_all(df, target, train_a=1, train_par=list(range(0,50)), test_par=list(range(50,75))):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #Select proper participants for training\n",
    "    df_train = df.loc[df['PID'].isin(train_par)]\n",
    "    #Select only musical sections\n",
    "    df_train = df_train.iloc[(int(len(df_train)/2)):,:]\n",
    "    #columns to drop from X\n",
    "    drop_cols = ['PID', 'SID', 'target_1_sec', 'X', 'Y','Z', 'segment']\n",
    "    #set milliseconds as index\n",
    "    df_train = df_train.set_index('millisecond')\n",
    " \n",
    "    #\n",
    "    train_X = df_train.drop(columns=drop_cols)#.to_numpy()\n",
    "    train_y = df_train[target]#.to_numpy()\n",
    "    \n",
    "    #Select proper participants for training\n",
    "    df_test = df.loc[df['PID'].isin(test_par)]\n",
    "    #Select only musical sections\n",
    "    df_test = df_test.iloc[(int(len(df_train)/2)):,:]\n",
    "    #columns to drop from X\n",
    "    drop_cols = ['PID', 'SID', 'target_1_sec', 'X', 'Y','Z', 'segment']\n",
    "    #set milliseconds as index\n",
    "    df_test = df_test.set_index('millisecond')\n",
    "    \n",
    "    \n",
    "    test_X = df_test.drop(columns=drop_cols)\n",
    "    test_y = df_test.loc[:,[target,'PID']]\n",
    "        \n",
    "    return train_X, test_X, train_y, test_y, train_par, test_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_prep import Xy_prep_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run an out of the box elasticNet on the PCA transformed model_data to see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prep/gfn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prep/gfn.py\n",
    "\n",
    "#From https://johaupt.github.io/scikit-learn/tutorial/python/data%20processing/ml%20pipeline/\n",
    "#model%20interpretation/columnTransformer_feature_names.html\n",
    "\n",
    "def get_feature_name(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    import warnings\n",
    "    import sklearn\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_prep import get_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [26, 54, 5, 1, 16, 6, 18, 17, 44, 19], 'test': [40, 12, 28, 39, 37]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars = list(model_data['PID'].unique())\n",
    "#max 53\n",
    "train_par = {x:pars[x] for x in np.random.choice(range(len(pars)), size=10, replace=False)}\n",
    "for x in sorted(train_par.keys(), reverse=True):\n",
    "    pars.pop(x)\n",
    "#max 20\n",
    "test_par={x:pars[x] for x in np.random.choice(range(len(pars)), size=5, replace=False)}\n",
    "participants = {'train':list(train_par.values()), 'test':list(test_par.values())} \n",
    "participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y, train_par, test_par = Xy_prep_all(model_data, 'target_1_sec', \n",
    "                                                            train_par=participants['train'], test_par=participants['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create column lists to use as inputs for the StandardScaler and OneHotEncoder Pipeline\n",
    "num_cols = list(train_X.columns[-10:])\n",
    "num_cols.extend(['Height', 'Age', 'Listen', 'Produce', 'Dance', 'Exercise'])\n",
    "cat_cols = ['Tiresome', 'age_bin', 'height_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up instances of StanardSCaler, OneHotEncoder and ColumnTransformer to process model_data\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "CT = ColumnTransformer(remainder='passthrough', transformers=[('scaler', scaler, num_cols),\n",
    "                                              ('ohe', ohe, cat_cols)], verbose=True, sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ........ (1 of 3) Processing scaler, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 3) Processing ohe, total=   0.0s\n",
      "[ColumnTransformer] ..... (3 of 3) Processing remainder, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bwils\\OneDrive\\Documents\\GitHub\\MicroMotion\\model_prep\\gfn.py:36: UserWarning: Transformer scaler (type StandardScaler) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n"
     ]
    }
   ],
   "source": [
    "train_Xct = CT.fit_transform(train_X)\n",
    "    \n",
    "#transform test data based on training fit\n",
    "test_Xct = CT.transform(test_X)\n",
    "\n",
    "#get columns names from ColumnTransformer\n",
    "cols = get_feature_name(CT)\n",
    "train_Xf = pd.DataFrame(train_Xct, columns=cols)\n",
    "cols = get_feature_name(CT)\n",
    "test_Xf = pd.DataFrame(test_Xct, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xf.to_pickle(r'DFs/train_test/train_X.pkl')\n",
    "train_y.to_pickle(r'DFs/train_test/train_y.pkl')\n",
    "test_Xf.to_pickle(r'DFs/train_test/test_X.pkl')\n",
    "test_y.to_pickle(r'DFs/train_test/test_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'DFs/train_test/participants.pkl','wb')\n",
    "pickle.dump(participants,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micromotion",
   "language": "python",
   "name": "micromotion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
