{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Scheduler: \"tcp://127.0.0.1:60353\" workers: 4 cores: 8, tasks: 0>,\n",
       " {0: <Nanny: tcp://127.0.0.1:60381, threads: 2>,\n",
       "  1: <Nanny: tcp://127.0.0.1:60384, threads: 2>,\n",
       "  2: <Nanny: tcp://127.0.0.1:60378, threads: 2>,\n",
       "  3: <Nanny: tcp://127.0.0.1:60375, threads: 2>})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "import joblib\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "cluster.scheduler, cluster.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "model_data = pd.read_pickle('DFs/model_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Functions\n",
    "Create functions for preparing data to be model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prep\\all_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prep\\all_prep.py \n",
    "\n",
    "def Xy_prep_all(df, target, train_a=1, train_par=list(range(0,50)), test_par=list(range(50,75))):\n",
    "    #Takes in full dataset and outputs train and test sets based on input list of participants\n",
    "    #for training on and testing on\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #Select proper participants for training\n",
    "    df_train = df.loc[df['PID'].isin(train_par)]\n",
    "    #Select only musical sections\n",
    "    df_train = df_train.iloc[(int(len(df_train)/2)):,:]\n",
    "    #columns to drop from X\n",
    "    drop_cols = ['PID', 'SID', 'target_1_sec', 'X', 'Y','Z', 'segment']\n",
    "    #set milliseconds as index\n",
    "    df_train = df_train.set_index('millisecond')\n",
    " \n",
    "    #Create train datasets for X and y\n",
    "    train_X = df_train.drop(columns=drop_cols)\n",
    "    train_y = df_train[target]\n",
    "    \n",
    "    #Select proper participants for training\n",
    "    df_test = df.loc[df['PID'].isin(test_par)]\n",
    "    #Select only musical sections\n",
    "    df_test = df_test.iloc[(int(len(df_train)/2)):,:]\n",
    "    #columns to drop from X\n",
    "    drop_cols = ['PID', 'SID', 'target_1_sec', 'X', 'Y','Z', 'segment']\n",
    "    #set milliseconds as index\n",
    "    df_test = df_test.set_index('millisecond')\n",
    "    \n",
    "    #Create test datasets for X and y\n",
    "    test_X = df_test.drop(columns=drop_cols)\n",
    "    test_y = df_test.loc[:,[target,'PID']]\n",
    "        \n",
    "    return train_X, test_X, train_y, test_y, train_par, test_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_prep import Xy_prep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prep/gfn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prep/gfn.py\n",
    "\n",
    "#From https://johaupt.github.io/scikit-learn/tutorial/python/data%20processing/ml%20pipeline/\n",
    "#model%20interpretation/columnTransformer_feature_names.html\n",
    "\n",
    "def get_feature_name(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    import warnings\n",
    "    import sklearn\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_prep import get_feature_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particpant Selection\n",
    "To reduce computation time we will be using 10 participants as training data and 5 other participants as testing data. The participants will both be chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [43, 32, 29, 1, 9, 5, 20, 67, 45, 35], 'test': [8, 11, 34, 6, 4]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#make list of unique participant numbers\n",
    "pars = list(model_data['PID'].unique())\n",
    "\n",
    "#randmoly select train participants, max 53\n",
    "train_par = {x:pars[x] for x in np.random.choice(range(len(pars)), size=10, replace=False)}\n",
    "\n",
    "#remove train participants from list of participants\n",
    "for x in sorted(train_par.keys(), reverse=True):\n",
    "    pars.pop(x)\n",
    "\n",
    "#randomly select test participants, max 20\n",
    "test_par={x:pars[x] for x in np.random.choice(range(len(pars)), size=5, replace=False)}\n",
    "\n",
    "#Create dictionary with lists of train and test participants\n",
    "participants = {'train':list(train_par.values()), 'test':list(test_par.values())} \n",
    "participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run prep functions to create train/test sets\n",
    "train_X, test_X, train_y, test_y, train_par, test_par = Xy_prep_all(model_data, 'target_1_sec', \n",
    "                                                            train_par=participants['train'], test_par=participants['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create column lists to use as inputs for the StandardScaler and OneHotEncoder Pipeline\n",
    "num_cols = list(train_X.columns[-10:])\n",
    "num_cols.extend(['Height', 'Age', 'Listen', 'Produce', 'Dance', 'Exercise'])\n",
    "cat_cols = ['Tiresome', 'age_bin', 'height_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up instances of StanardSCaler, OneHotEncoder and ColumnTransformer to process model_data\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "CT = ColumnTransformer(remainder='passthrough', transformers=[('scaler', scaler, num_cols),\n",
    "                                              ('ohe', ohe, cat_cols)], verbose=True, sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ........ (1 of 3) Processing scaler, total=   0.1s\n",
      "[ColumnTransformer] ........... (2 of 3) Processing ohe, total=   0.0s\n",
      "[ColumnTransformer] ..... (3 of 3) Processing remainder, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bwils\\OneDrive\\Documents\\GitHub\\MicroMotion\\model_prep\\gfn.py:36: UserWarning: Transformer scaler (type StandardScaler) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n"
     ]
    }
   ],
   "source": [
    "#fit column transformer on train data\n",
    "train_Xct = CT.fit_transform(train_X)\n",
    "    \n",
    "#transform test data based on training fit\n",
    "test_Xct = CT.transform(test_X)\n",
    "\n",
    "#get columns names from ColumnTransformer\n",
    "cols = get_feature_name(CT)\n",
    "train_Xf = pd.DataFrame(train_Xct, columns=cols)\n",
    "cols = get_feature_name(CT)\n",
    "test_Xf = pd.DataFrame(test_Xct, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "pca.fit(train_Xf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance_explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.283894e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.462982e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.796775e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.976103e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.341243e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.754356e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.294344e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.956937e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.465278e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.259378e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.592875e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.165910e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.334222e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.049624e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.792325e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.469348e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.429243e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.175069e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.101871e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.091621e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.455542e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.124024e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.836972e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.889451e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.478926e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    variance_explained\n",
       "0         9.283894e-01\n",
       "1         6.462982e-02\n",
       "2         6.796775e-03\n",
       "3         5.976103e-05\n",
       "4         2.341243e-05\n",
       "5         1.754356e-05\n",
       "6         9.294344e-06\n",
       "7         8.956937e-06\n",
       "8         6.465278e-06\n",
       "9         5.259378e-06\n",
       "10        4.592875e-06\n",
       "11        3.165910e-06\n",
       "12        2.334222e-06\n",
       "13        2.049624e-06\n",
       "14        1.792325e-06\n",
       "15        1.469348e-06\n",
       "16        1.429243e-06\n",
       "17        1.175069e-06\n",
       "18        1.101871e-06\n",
       "19        1.091621e-06\n",
       "20        9.455542e-07\n",
       "21        9.124024e-07\n",
       "22        8.836972e-07\n",
       "23        7.889451e-07\n",
       "24        7.478926e-07"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = pca.explained_variance_ratio_\n",
    "exdf = pd.DataFrame(ex.reshape(1, -1), columns=range(len(ex)))\n",
    "exdf.T.rename(columns={0:'variance_explained'}).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output data for model\n",
    "train_Xf.to_pickle(r'DFs/train_test/train_X.pkl')\n",
    "train_y.to_pickle(r'DFs/train_test/train_y.pkl')\n",
    "test_Xf.to_pickle(r'DFs/train_test/test_X.pkl')\n",
    "test_y.to_pickle(r'DFs/train_test/test_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dictionary of train and test lists\n",
    "f = open(r'DFs/train_test/participants.pkl','wb')\n",
    "pickle.dump(participants,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micromotion",
   "language": "python",
   "name": "micromotion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
